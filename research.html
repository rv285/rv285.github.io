<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Ramji Venkataramanan</title>
<meta http-equiv="Content-Language" content="English" />
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<link rel="stylesheet" type="text/css" href="style2.css" media="screen" />
</head>

<body>
<div id="wrap">

<div id="header">
<h1><a href="#">Ramji Venkataramanan</a></h1>
<br>
</div>

<div id="navigation">
<ul>
<li><a href="./index.html">Home</a></li>
<li><a href="./bio.html">Bio</a></li>
<li><a href="./research.html">  <font color="black">Research<font></a></li>
<li><a href="./pub.html">Publications</a></li>
<li><a href="./teaching.html">Teaching</a></li>
<li><a href="./group.html">Group</a></li>
</ul>
</div>


<div id="content">

<div id="left">

<p>
My research interests are broadly in statistical learning,  information theory, and communications.   Here are some representative papers and slides. </p>
<!--
<ul>
<li> <a href="#hdim">Statistical learning in high dimensions</a> </li>
<li> <a href="#sparcs">Communication and compression using sparse regression codes</a> </li>
</ul>
<p> A description of some  past projects can be found  <a href="./research_prev.html" target=blank>here</a>. </p>
-->

<br>
<a name="hdim"></a>
<h2>Statistical learning</h2>
<!--
<p><b>Approximate Message Passing (AMP) algorithms</b></p>
<p> AMP algorithms achieve state-of-the-art performance for several high-dimensional statistical estimation problems, including compressed sensing and low-rank matrix estimation. Our recent work includes  AMP algorithms for low-rank matrix estimation, for estimation in generalized linear model, and non-asymptotic guarantees on the performance of AMP. </p>
Some representative papers and slides:
-->
<ul>
<li> R. Venkataramanan, K. K&#246gler, and M. Mondelli "Estimation in Rotationally Invariant Generalized Linear Models via  Approximate Message Passing". [<a href="http://arxiv.org/abs/2112.04330" target="_blank">PDF</a>] 
</li>

<li> M. Mondelli and R. Venkataramanan, "PCA Initialization for Approximate Message Passing in Rotationally Invariant Models" NeurIPS 2021.  
[<a href="https://arxiv.org/abs/2106.02356" target="_blank">PDF</a>]  </li>


<li>O. Feng, R. Venkataramanan, C. Rush, R. Samworth, "A unifying tutorial on Approximate Message Passing" (2021). [<a href="https://arxiv.org/abs/2105.02180" target="_blank">PDF</a>]  </li>

<li> M. Mondelli and R. Venkataramanan, "Approximate Message Passing with Spectral Initialization for Generalized Linear Models", AISTATS 2021.  
[<a href="https://arxiv.org/abs/2010.03460" target="_blank">PDF</a>]  </li>

<li> M. Mondelli, C. Thrampoulidis, R. Venkataramanan, "Optimal Combination of Linear and Spectral Estimators for Generalized Linear Models",   <i>Foundations of Computational Mathematics</i>  (2021+).  [<a href="https://arxiv.org/abs/2008.03326" target="_blank">PDF</a>]  </li>

<li>  A. Montanari and R. Venkataramanan, "Estimation of Low-Rank Matrices via Approximate Message Passing",  <i>Annals of Statistics</i>, vol. 49, no. 1, pp. 321-345, February 2021.  [<a href="https://arxiv.org/abs/1711.01682">PDF</a>] [<a href="CCIMI_low_rank.pdf">Slides </a> from CCIMI seminar, 2018]</li>

<!--
<li> C. Rush and R. Venkataramanan, "Finite Sample Analysis of Approximate Message Passing Algorithms", <i>IEEE Transactions on Information Theory</i>,  vol. 64, no. 11, pp. 7264-7286, November 2018. [<a href="https://arxiv.org/abs/1606.01800">PDF</a>] </li> 
-->

<li> R. Venkataramanan and O. Johnson, "A strong converse bound for multiple hypothesis testing, with applications to high-dimensional estimation", <i>Electronic Journal of Statistics</i>, Vol. 12, No. 1, pp. 1126-1149, 2018. 
[<a href="https://arxiv.org/abs/1706.04410">PDF</a>]  [<a href="beyondIID_RV_strong_conv.pdf">Slides</a> from talk at Beyond IID '18] </li> </ul>

<br><br>

<!--
<p> <b>Shrinkage estimation in high dimensions </b> </p>
<!--
<p>  Consider the problem of estimating a high-dimensional vector of parameters  &#952 from a noisy observation.  Shrinkage estimators (that shrink the data towards a target vector or subspace) are known to dominate the simple maximum-likelihood (ML) estimator for this problem. However,  the gains over ML are substantial only when &#952 happens to lie close to the target subspace. This leads to the question: how do we design estimators that give significant risk reduction over ML for a wide range of  &#952? </p>
<p>
In this work, we  infer the clustering structure of &#952 from the data, and use it to design  target subspaces  tailored to &#952. As the dimension grows, these shrinkage estimators  give substantial risk reduction over ML for a wide range of  &#952.  We have also used these ideas to design good estimators for the setting where it is known that &#952 is sparse, but no other prior information is available. 
</p>
-->

<!--
<ul>
 <li> "Cluster-seeking James-Stein Estimators",  <i>IEEE Transactions on Information Theory</i>, vol. 64, no. 2, pp. 853-874, February 2018. [<a href="http://arxiv.org/abs/1602.00542">PDF</a>] [<a href="ita16_shrinkage_talk.pdf">Slides</a> from ITA '16]</li>

 <li> "Empirical Bayes estimators for high-dimensional sparse vectors", <i> 
 Information and Inference: A Journal of the IMA</i> vol. 9, no. 1, pp. 195-234, March 2020. [<a href="https://academic.oup.com/imaiai/article/9/1/195/5320816?guestAccessKey=97bd26dd-eaf3-49d3-9b4d-1c45ed2abb83" target="_blank">PDF</a>] 
 [<a href="eBayes_simulation_reproducibles.zip"> Code</a>] 
 [<a href="eBayes_ISIT18_talk.pdf">Slides</a> from ISIT '18]</li>
</ul>
<br>
-->

<!-- <p>
Here we propose a hybrid estimator that reliably picks the better among  two sparse estimators based on risk estimates computed from the data. </p> -->
<!--
<p> <b>Converse bounds for high-dimensiona estimation</b> </p>
We obtain new lower bounds on the minimax risk of inference problems such as density estimation, active learning, and compressed sensing. These bounds give tighter "strong converse" results in contrast to the weak converses provided by Fano's inequality. 
<ul>
<li> "A strong converse bound for multiple hypothesis testing, with applications to high-dimensional estimation", <i>Electronic Journal of Statistics</i>, Vol. 12, No. 1, pp. 1126-1149, 2018. 
[<a href="https://arxiv.org/abs/1706.04410">PDF</a>]  [<a href="beyondIID_RV_strong_conv.pdf">Slides</a> from talk at Beyond IID '18] </li> </ul>
<br><br>
-->





<a name="sparcs"></a>
<h2>Sparse Regression Codes</h2>
<!--
<p> <b>Sparse regression codes for communication and compression</b></p>
<p>     Sparse regression codes were recently  <a href="http://arxiv.org/abs/1006.3870">proposed</a>  for efficient communication over channels with additive Gaussian noise.  We have  developed a low-complexity <a href="http://arxiv.org/abs/1501.05892">capacity-achieving decoder</a> for these codes based on "approximate message passing" (AMP) techniques. Using the sparse regression framework, we have also designed efficient algorithms for lossy data compression which attain  the optimal  compression rate  for Gaussian sources.  Furthermore, these source and channel codes can be combined to obtain  rate-optimal codes for several canonical models in network information theory. </p>

<li>   ISIT'16 tutorial on Sparse Regression Codes:  <a href="https://goo.gl/8H8wrk" target="_blank"> handout + slides </a></li>
-->
<ul>
<li>   R. Venkataramanan, S. Tatikonda and A. Barron, "Sparse Regression Codes", <i>Foundations and Trends in Communications and Information Theory</i>, vol. 15, no. 1-2, pp. 1-195, 2019. [<a href="https://arxiv.org/abs/1911.00771" target="_blank">PDF</a>] </li>

<li> K. Hsieh, C. Rush, and R. Venkataramanan, "Near-optimal coding for many-user multiple access channels" [<a href="https://arxiv.org/abs/2102.04730" target="_blank">PDF</a>] 

<li> C. Rush, K. Hsieh, and R. Venkataramanan, "Capacity-achieving spatially coupled sparse superposition codes with AMP decoding",    <i>IEEE Transactions on Information Theory</i>, vol. 67, no. 7, pp. 4446 - 4484, July 2021. [<a href="https://arxiv.org/abs/2002.07844" target="_blank">PDF</a>] </li>

<li> K. Hsieh, and R. Venkataramanan, "Modulated sparse superposition codes for the complex AWGN channel", <i>IEEE Transactions on Information Theory</i>, vol. 67, no. 7, pp. 4385 - 4404, July 2021. [<a href="https://arxiv.org/abs/2004.09549" target="_blank">PDF</a>] [<a href="ModSPARC_isit2020_slides.pdf">Slides</a> from Kuan's ISIT'20 talk] </li>

</ul>
<br><br>

<!--
<br>
<li> "Capacity-achieving Sparse Superposition Codes with Approximate Message Passing Decoding", <i>IEEE Transactions on Information Theory</i>, March 2017.  [<a href="http://arxiv.org/abs/1501.05892">PDF</a>] </li> 
<li> C. Rush and R. Venkataramanan, "The error probability of sparse superposition codes with Approximate Message Passing decoding", <i>IEEE Transactions on Information Theory</i>, May 2019. [<a href="https://arxiv.org/abs/1712.06866">PDF</a>] 
<li>  "Techniques for improving the finite length performance of sparse superposition codes",  <i>IEEE Transactions on Communications</i>, vol. 66, no. 3, pp.905-917, March 2018.   [<a href="https://arxiv.org/abs/1705.02091">PDF</a>] </li>

<p> <b>Sparse regression codes for compression</b> </p>
<p> Using the sparse regression framework, we have also designed efficient algorithms for lossy data compression which attain  the optimal  compression rate (the rate-distortion function) for Gaussian sources.  Furthermore, the source and channel codes constructed above can be combined to obtain  rate-optimal codes for several canonical models in network information theory.
</p>
<ul>

<li> "Lossy Compression via Sparse Linear Regression: Computationally Efficient Encoding and Decoding", <i>IEEE Transactions on Information Theory</i>, vol. 60, no. 6, pp. 3265-3278, June 2014.  [<a href="http://arxiv.org/abs/1212.1707">PDF</a>]  
[<a href="ita13_sparc_talk.pdf">Slides</a> from ITA '13]</li>

<li>  "Lossy Compression via Sparse Linear Regression: Performance under Minimum-distance Encoding", <i>IEEE Transactions on Information Theory</i>, vol. 60, no. 6, pp. 3254-3264, June 2014. [<a href="http://arxiv.org/abs/1202.0840">PDF</a>]   [<a href="isit_sparc_talk.pdf">Slides</a> from ISIT '12]  </li>

<li>  "The Rate-Distortion Function and Excess-Distortion Exponent of Sparse Regression Codes with Optimal Encoding", <i>IEEE Transactions on Information Theory</i>, vol. 63, no. 8, pp. 5228-5243 ,  August 2017. [<a href="https://arxiv.org/abs/1401.5272">PDF</a>] [<a href="isit14_sparc_talk.pdf">Slides</a> from ISIT '14]  </li>

<li>   "Sparse Regression Codes for Multi-terminal Source and Channel Coding",  Allerton 2012.
[<a href="allerton_sparc_final.pdf">PDF</a>]  [<a href="allerton_sparc_talk.pdf">Slides</a>]  </li>
</ul>
-->




<!--
<a name="indel"></a>
<h2>Codes for channels with deletions and insertions </h2>
<p> <b>Low-complexity codes for deletions and insertions </b> </p>
<p>
The problem of synchronization from  deletions and insertions is important in several applications, such as file sharing and distributed editing. We have designed  low-complexity rate-efficient codes for various edit models with deletions and insertions. </p>
<ul>
<li> "Multilayer codes for synchronization from deletions", ITW 2017 [<a href="https://arxiv.org/abs/1705.06670">PDF</a>]</li>
<li>"Efficient systematic encoding of non-binary VT codes", ISIT 2018. [<a href="https://arxiv.org/abs/1708.04071">PDF</a>] </li> 
<li> "Coding for segmented edit channels",  <i>IEEE Transactions on Information Theory</i>,  April 2018. [<a href="https://arxiv.org/abs/1701.06341">PDF</a>] </li>
<li>  "Low-Complexity Interactive Algorithms for Synchronization from Deletions, Insertions, and Substitutions", <i>IEEE Transactions on Information Theory</i>, vol. 61, no. 10, pp. 5670-5689, October 2015. [<a href="http://arxiv.org/abs/1310.2026">PDF</a>]</li>
<li> <a href="ramji_banff_talk.pdf">Slides</a> from talk at the Banff Workshop on Interactive Information Theory, Jan. 2012. </li>
</ul>
<br>
<p> <b>Capacity bounds for deletion and insertion channels </b> </p>
<ul>
<li>  "Achievable Rates for Channels with Deletions and Insertions", <i>IEEE Transactions on Information Theory</i>, vol. 59, no.11,  pp. 6990-7013, November 2013.  [<a href="ITNov13_indel.pdf">PDF</a>]
  [<a href="itw13_indel_talk.pdf">Slides</a> from talk at ITW 2013]. </li>
 </ul>
<br><br>
-->




<!--
<li> <a href="#indel">Codes for  channels with deletions and insertions</a> </li>
<p>
<b> <i> Network Modeling.</i></b></p>
<p> My ongoing work in stochastic networks aims to build good models for data traffic
in large networks. There was a large body of work in the 1990s which showed that packet
traffic in the internet was <i>long-range dependent</i>. This was strong evidence against using traditional Poisson models
for the internet.   Internet speeds, however, have increased more than a thousand-fold since the early 90s.  We propose and analyze
a succinct model which captures the essential  packet and flow level features of the network as the link capacities grow large. Analysis on this model indicates that Poisson processes are
good approximations for aggregate packet traffic in high-speed networks.
</p> <br>
<ul>
<li> "A Flow- and Packet-level Model of the Internet",  [<a href="mgi_isit_version.pdf">PDF</a>]
</li> </ul>
-->

<!--<br>
<p>Another area of interest is algorithms for inference on graphical models. I got interested in this topic during a summer internship at <a href="http://www.merl.com">MERL</a>, where I worked on
designing error-correcting codes for optical networks with
<a href="http://jonathanyedidia.wordpress.com/">Dr. Jonathan Yedidia</a>.
</p>
-->

<div class="menubottom"> </div>
</div>

<div style="clear:both;"> </div>

</div>

<div id="bottom"> </div>

<!--<div id="footer">
&copy; Copyright 2009 by <a href="#">Ramji Venkataramanan</a> | Design by <a href="http://www.minimalistic-design.net">Minimalistic Design</a>
</div>-->
</div>
</body>
</html>
